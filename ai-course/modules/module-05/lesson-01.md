# Урок 25: Нейронные сети для компьютерного зрения

## Информация об уроке
- **Модуль:** 5 - Обработка данных и пайплайны
- **Продолжительность:** 160 минут
- **Тип:** Теоретический + Практический

## Содержание урока

# Урок 25: Нейронные сети для компьютерного зрения

## Введение в глубокое обучение для компьютерного зрения

Революция в области компьютерного зрения, произошедшая в последнее десятилетие, неразрывно связана с развитием глубоких нейронных сетей. Для AI-архитектора понимание принципов работы, архитектур и особенностей применения нейронных сетей в задачах компьютерного зрения является критически важным для создания эффективных промышленных систем визуального анализа.

Глубокие нейронные сети кардинально изменили подход к решению задач компьютерного зрения, обеспечив качественный скачок в точности распознавания объектов, сегментации изображений и других задач визуального анализа. В отличие от традиционных методов, требующих ручного проектирования признаков, глубокие сети способны автоматически извлекать иерархические представления из данных, что делает их особенно эффективными для сложных задач анализа изображений.

Промышленные применения нейронных сетей для компьютерного зрения охватывают широкий спектр задач, от автоматизированного контроля качества до роботизированного зрения и мониторинга производственных процессов. Успешное внедрение таких систем требует глубокого понимания как теоретических основ глубокого обучения, так и практических аспектов их применения в условиях реального производства.

Основные преимущества нейронных сетей в компьютерном зрении включают способность к автоматическому извлечению признаков, высокую точность распознавания, устойчивость к вариациям в данных и возможность обучения на больших объемах данных. Однако их применение также связано с определенными вызовами, включая требования к вычислительным ресурсам, необходимость больших объемов размеченных данных и сложность интерпретации результатов.

### Основы сверточных нейронных сетей

Сверточные нейронные сети (CNN) представляют собой специализированную архитектуру глубоких нейронных сетей, специально разработанную для обработки данных с сеточной топологией, таких как изображения. Ключевые принципы CNN основаны на трех важных идеях: локальные связи, разделяемые веса и пространственная иерархия.

**Операция свертки** является основным строительным блоком CNN. Математически двумерная свертка определяется как:

(f * g)(x,y) = ΣΣ f(m,n) * g(x-m, y-n)

где f представляет входное изображение, g - ядро свертки (фильтр), а результат представляет карту признаков. В контексте нейронных сетей операция свертки обычно реализуется как кросс-корреляция:

S(i,j) = ΣΣ I(i+m, j+n) * K(m,n)

где I - входное изображение, K - ядро свертки, S - выходная карта признаков.

Сверточные слои применяют множество фильтров к входному изображению, каждый из которых настроен на обнаружение определенных визуальных паттернов. На ранних слоях сети фильтры обычно обнаруживают простые признаки, такие как границы, углы и текстуры. По мере углубления в сеть фильтры становятся более сложными и способными обнаруживать высокоуровневые семантические признаки.

**Пулинг (субдискретизация)** используется для уменьшения пространственных размеров карт признаков, что снижает вычислительную нагрузку и обеспечивает некоторую инвариантность к небольшим трансляциям. Максимальный пулинг выбирает максимальное значение в каждом окне:

y(i,j) = max{x(i*s+m, j*s+n) | 0 ≤ m,n < k}

где s - шаг пулинга, k - размер окна пулинга.

Средний пулинг вычисляет среднее значение в окне, что может быть полезно для сохранения информации о фоне. Адаптивный пулинг автоматически подстраивает размер окна для получения выходных карт признаков заданного размера.

**Функции активации** вводят нелинейность в сеть, что критически важно для способности сети аппроксимировать сложные функции. ReLU (Rectified Linear Unit) является наиболее популярной функцией активации в CNN:

f(x) = max(0, x)

ReLU обеспечивает быстрое обучение и помогает решить проблему исчезающих градиентов. Варианты ReLU включают Leaky ReLU, ELU (Exponential Linear Unit) и Swish, каждый из которых имеет свои преимущества в определенных контекстах.

### Классические архитектуры CNN

Развитие архитектур CNN прошло через несколько важных этапов, каждый из которых внес значительный вклад в понимание принципов эффективного проектирования глубоких сетей для компьютерного зрения.

**LeNet-5**, разработанная Яном ЛеКуном в 1998 году, стала первой успешной CNN для распознавания рукописных цифр. Архитектура включала чередующиеся сверточные и пулинг слои, за которыми следовали полносвязные слои. Несмотря на простоту, LeNet-5 заложила основные принципы проектирования CNN.

**AlexNet** (2012) стала прорывной архитектурой, которая продемонстрировала эффективность глубоких CNN на крупномасштабных задачах распознавания изображений. Ключевые инновации включали использование ReLU активации, dropout для регуляризации, и обучение на GPU. Архитектура состояла из 8 слоев (5 сверточных и 3 полносвязных) и показала значительное улучшение точности на датасете ImageNet.

**VGGNet** (2014) исследовала влияние глубины сети на производительность, используя очень маленькие (3×3) сверточные фильтры. Архитектуры VGG-16 и VGG-19 показали, что увеличение глубины сети при использовании маленьких фильтров может значительно улучшить производительность. Принцип использования стеков маленьких фильтров вместо больших стал стандартной практикой.

**GoogLeNet/Inception** (2014) представила концепцию модулей Inception, которые применяют свертки различных размеров параллельно и объединяют результаты. Это позволило создать более эффективные сети с меньшим количеством параметров. Архитектура также использовала глобальный средний пулинг вместо полносвязных слоев, что значительно уменьшило количество параметров.

**ResNet** (2015) решила проблему деградации глубоких сетей с помощью остаточных соединений (skip connections). Остаточный блок определяется как:

y = F(x) + x

где F(x) представляет остаточное отображение, которое сеть должна изучить. Это позволило обучать очень глубокие сети (до 152 слоев) без деградации производительности.

### Современные архитектуры и инновации

Современные архитектуры CNN продолжают эволюционировать, включая новые принципы проектирования и оптимизации для различных применений.

**DenseNet** расширяет идею остаточных соединений, соединяя каждый слой со всеми последующими слоями в плотном блоке:

x_l = H_l([x_0, x_1, ..., x_{l-1}])

где [x_0, x_1, ..., x_{l-1}] представляет конкатенацию карт признаков всех предыдущих слоев. Это обеспечивает максимальный поток информации между слоями и улучшает градиентный поток.

**MobileNet** оптимизирована для мобильных и встраиваемых устройств, используя глубинно-разделимые свертки (depthwise separable convolutions). Стандартная свертка разделяется на две операции: глубинную свертку (применяется к каждому каналу отдельно) и точечную свертку (1×1 свертка для объединения каналов). Это значительно уменьшает количество параметров и вычислений.

**EfficientNet** использует составное масштабирование для одновременного увеличения глубины, ширины и разрешения сети сбалансированным образом. Коэффициент масштабирования определяется как:

depth: d = α^φ
width: w = β^φ  
resolution: r = γ^φ

где α, β, γ - константы, определяемые через поиск по сетке, а φ - составной коэффициент.

**Vision Transformer (ViT)** адаптирует архитектуру трансформера для задач компьютерного зрения, разделяя изображение на патчи и обрабатывая их как последовательность токенов. Несмотря на отсутствие индуктивных смещений CNN, ViT показывает отличные результаты при обучении на больших датасетах.

### Специализированные архитектуры для промышленных задач

Промышленные применения часто требуют специализированных архитектур, оптимизированных для конкретных задач и ограничений.

**Архитектуры для обнаружения объектов** решают задачу одновременного определения местоположения и классификации объектов в изображении. Двухэтапные детекторы, такие как R-CNN, Fast R-CNN и Faster R-CNN, сначала генерируют предложения регионов, а затем классифицируют их. Faster R-CNN использует Region Proposal Network (RPN) для генерации предложений:

p_i = σ(W_cls * φ_i + b_cls)
t_i = W_reg * φ_i + b_reg

где φ_i - признаки i-го якоря, p_i - вероятность объекта, t_i - координаты ограничивающего прямоугольника.

Одноэтапные детекторы, такие как YOLO и SSD, выполняют обнаружение за один проход сети, что обеспечивает высокую скорость работы. YOLO разделяет изображение на сетку и предсказывает ограничивающие прямоугольники и вероятности классов для каждой ячейки сетки.

**Архитектуры для семантической сегментации** присваивают метку класса каждому пикселю изображения. FCN (Fully Convolutional Networks) заменяет полносвязные слои сверточными, что позволяет обрабатывать изображения произвольного размера. U-Net использует энкодер-декодер архитектуру с skip connections для сохранения пространственной информации:

Энкодер: x_i = f_i(x_{i-1})
Декодер: y_i = g_i([y_{i+1}, x_{n-i}])

где [y_{i+1}, x_{n-i}] представляет конкатенацию восходящего пути и соответствующего skip connection.

**Архитектуры для контроля качества** часто используют специализированные потери и архитектуры для обнаружения дефектов. Siamese networks сравнивают изображения попарно, что полезно для обнаружения аномалий:

d = ||f(x_1) - f(x_2)||_2

где f представляет сеть признаков, а d - расстояние между представлениями.

Автоэнкодеры используются для обнаружения аномалий путем реконструкции нормальных изображений. Аномалии обнаруживаются по высокой ошибке реконструкции:

L = ||x - D(E(x))||_2

где E - энкодер, D - декодер, x - входное изображение.

## Обучение нейронных сетей для компьютерного зрения

Эффективное обучение нейронных сетей для задач компьютерного зрения требует понимания различных аспектов процесса обучения, от подготовки данных до оптимизации гиперпараметров.

### Подготовка данных и аугментация

Качество и количество обучающих данных критически важны для успешного обучения нейронных сетей. Промышленные применения часто сталкиваются с ограниченными объемами размеченных данных, что требует специальных подходов к подготовке и расширению датасетов.

**Предобработка изображений** включает нормализацию, изменение размера и коррекцию цвета. Нормализация обычно выполняется путем вычитания среднего и деления на стандартное отклонение:

x_norm = (x - μ) / σ

где μ и σ вычисляются по обучающему датасету. Для предобученных сетей используются статистики ImageNet: μ = [0.485, 0.456, 0.406], σ = [0.229, 0.224, 0.225] для каналов RGB.

**Аугментация данных** искусственно увеличивает размер датасета путем применения различных преобразований к исходным изображениям. Основные типы аугментации включают:

Геометрические преобразования: повороты, отражения, масштабирование, сдвиги. Поворот на угол θ описывается матрицей:

R(θ) = [cos(θ) -sin(θ); sin(θ) cos(θ)]

Фотометрические преобразования: изменение яркости, контраста, насыщенности, оттенка. Изменение яркости: I' = I + β, где β - параметр яркости.

Продвинутые методы аугментации включают Mixup, который создает виртуальные примеры путем линейной интерполяции:

x̃ = λx_i + (1-λ)x_j
ỹ = λy_i + (1-λ)y_j

где λ ~ Beta(α, α), обычно α = 0.2.

CutMix заменяет прямоугольную область одного изображения областью из другого изображения, сохраняя пропорциональность меток.

**Синтетические данные** становятся все более важными для промышленных применений. Генеративные модели, такие как GANs и VAEs, могут создавать реалистичные изображения для расширения датасетов. Процедурная генерация и симуляция также используются для создания контролируемых условий обучения.

### Функции потерь и метрики

Выбор подходящей функции потерь критически важен для успешного обучения нейронных сетей в задачах компьютерного зрения.

**Для задач классификации** наиболее распространенной является кросс-энтропийная потеря:

L = -Σ y_i * log(ŷ_i)

где y_i - истинная метка (one-hot), ŷ_i - предсказанная вероятность.

Focal Loss решает проблему дисбаланса классов, фокусируясь на сложных примерах:

FL(p_t) = -α_t(1-p_t)^γ log(p_t)

где p_t - вероятность истинного класса, α_t - весовой коэффициент, γ - параметр фокусировки.

**Для задач обнаружения объектов** используются комбинированные функции потерь, включающие потери классификации и локализации:

L = L_cls + λL_loc

где L_cls - потеря классификации, L_loc - потеря локализации (обычно Smooth L1), λ - весовой коэффициент.

IoU Loss напрямую оптимизирует метрику Intersection over Union:

L_IoU = 1 - IoU = 1 - (|A ∩ B|)/(|A ∪ B|)

**Для задач сегментации** используются пиксельные потери, такие как Dice Loss:

Dice = (2|X ∩ Y|)/(|X| + |Y|)
L_Dice = 1 - Dice

Tversky Loss обобщает Dice Loss, позволяя контролировать баланс между ложноположительными и ложноотрицательными результатами:

TI = TP/(TP + αFN + βFP)

где α и β - параметры, контролирующие важность различных типов ошибок.

### Техники оптимизации и регуляризации

Эффективная оптимизация нейронных сетей требует правильного выбора алгоритма оптимизации, скорости обучения и техник регуляризации.

**Алгоритмы оптимизации** эволюционировали от простого SGD к более сложным адаптивным методам. Adam комбинирует преимущества AdaGrad и RMSprop:

m_t = β_1 m_{t-1} + (1-β_1)g_t
v_t = β_2 v_{t-1} + (1-β_2)g_t^2
θ_{t+1} = θ_t - α(m̂_t)/(√v̂_t + ε)

где m̂_t и v̂_t - скорректированные на смещение оценки моментов.

AdamW исправляет проблемы Adam с weight decay:

θ_{t+1} = θ_t - α(m̂_t)/(√v̂_t + ε) - αλθ_t

**Планирование скорости обучения** критически важно для стабильного обучения. Cosine Annealing изменяет скорость обучения по косинусоидальному закону:

η_t = η_{min} + (η_{max} - η_{min})(1 + cos(πT_{cur}/T_{max}))/2

Warm-up постепенно увеличивает скорость обучения в начале обучения, что помогает стабилизировать процесс.

**Техники регуляризации** предотвращают переобучение и улучшают генерализацию. Dropout случайно обнуляет нейроны во время обучения:

y = f(x) ⊙ m

где m ~ Bernoulli(p), ⊙ - поэлементное произведение.

Batch Normalization нормализует входы каждого слоя:

BN(x) = γ((x - μ_B)/σ_B) + β

где μ_B и σ_B - среднее и стандартное отклонение по батчу, γ и β - обучаемые параметры.

Layer Normalization и Group Normalization предлагают альтернативы Batch Normalization для случаев с маленькими батчами или переменными размерами входов.

## Transfer Learning и Fine-tuning

Transfer Learning представляет собой мощную технику, особенно важную для промышленных применений, где размеченные данные часто ограничены.

### Принципы Transfer Learning

Transfer Learning основан на идее, что знания, полученные при решении одной задачи, могут быть полезны для решения связанной задачи. В контексте компьютерного зрения это означает использование сетей, предобученных на больших датасетах (например, ImageNet), для решения специфических задач.

**Иерархия признаков** в CNN делает Transfer Learning особенно эффективным. Ранние слои изучают низкоуровневые признаки (границы, текстуры), которые универсальны для большинства задач компьютерного зрения. Более глубокие слои изучают высокоуровневые, специфичные для задачи признаки.

Эффективность Transfer Learning зависит от сходства между исходной и целевой задачами, а также от размера целевого датасета. Чем больше сходство и меньше данных, тем больше пользы от Transfer Learning.

### Стратегии Fine-tuning

**Заморозка слоев** является базовой стратегией, где веса предобученных слоев остаются неизменными, а обучаются только новые слои:

θ_frozen = const
θ_new = θ_new - α∇L(θ_new)

**Постепенное размораживание** начинает с заморозки всех предобученных слоев и постепенно размораживает их, начиная с последних:

Эпоха 1-10: заморозить слои 1-N-2, обучать слои N-1, N
Эпоха 11-20: заморозить слои 1-N-3, обучать слои N-2, N-1, N

**Дифференциальные скорости обучения** используют разные скорости для разных частей сети:

θ_early = θ_early - α_low∇L(θ_early)
θ_late = θ_late - α_high∇L(θ_late)

где α_low << α_high.

**Циклическое обучение** (Cyclical Learning Rates) изменяет скорость обучения циклически, что может помочь избежать локальных минимумов и улучшить конвергенцию.

### Адаптация к промышленным задачам

Промышленные применения часто требуют специальной адаптации предобученных моделей.

**Доменная адаптация** решает проблему различий между обучающими и тестовыми данными. Adversarial Domain Adaptation использует состязательное обучение для изучения доменно-инвариантных признаков:

L = L_task + λL_domain

где L_domain - состязательная потеря для различения доменов.

**Few-shot Learning** позволяет обучать модели на очень малых количествах примеров. Meta-learning подходы, такие как MAML (Model-Agnostic Meta-Learning), изучают инициализацию, которая может быстро адаптироваться к новым задачам:

θ' = θ - α∇L_task(θ)
θ = θ - β∇Σ L_val(θ')

**Continual Learning** позволяет моделям изучать новые задачи без забывания предыдущих. Elastic Weight Consolidation (EWC) добавляет регуляризационный терм, который препятствует изменению важных весов:

L = L_new + λΣ F_i(θ_i - θ*_i)^2

где F_i - диагональ матрицы Фишера, θ*_i - оптимальные веса для предыдущих задач.

## Оптимизация и развертывание

Развертывание нейронных сетей в промышленных условиях требует оптимизации для обеспечения требуемой производительности и эффективности.

### Техники сжатия моделей

**Квантизация** уменьшает точность представления весов и активаций, что снижает требования к памяти и ускоряет вычисления. Post-training quantization применяется к уже обученной модели:

q = round(r/s) + z

где r - вещественное значение, s - масштабный коэффициент, z - нулевая точка, q - квантованное значение.

Quantization-aware training включает квантизацию в процесс обучения, что обычно дает лучшие результаты.

**Прунинг** удаляет менее важные веса или нейроны. Magnitude-based pruning удаляет веса с наименьшими абсолютными значениями:

W_pruned = W ⊙ M

где M - бинарная маска, M_ij = 1 если |W_ij| > threshold.

Structured pruning удаляет целые каналы или слои, что более эффективно для аппаратного ускорения.

**Дистилляция знаний** обучает компактную модель (студент) имитировать поведение большой модели (учитель):

L = αL_hard + (1-α)τ²L_soft

где L_hard - потеря на истинных метках, L_soft - потеря на мягких предсказаниях учителя, τ - температура.

### Аппаратная оптимизация

**GPU оптимизация** включает эффективное использование памяти и параллелизма. Смешанная точность (mixed precision) использует FP16 для большинства операций и FP32 для критических вычислений:

loss_scaled = loss * scale_factor
gradients = backward(loss_scaled) / scale_factor

**Специализированные ускорители**, такие как TPU, предназначены специально для операций машинного обучения. Они обеспечивают высокую пропускную способность для матричных операций.

**Edge deployment** требует оптимизации для ограниченных ресурсов. TensorRT, OpenVINO и другие фреймворки обеспечивают оптимизацию для различных аппаратных платформ.

### Мониторинг и обслуживание

**Мониторинг производительности** включает отслеживание точности, латентности и пропускной способности в реальном времени. Drift detection выявляет изменения в распределении данных:

KL(P||Q) = Σ P(x) log(P(x)/Q(x))

где P - распределение обучающих данных, Q - распределение новых данных.

**Автоматическое переобучение** запускается при обнаружении деградации производительности или значительного дрифта данных. Это может включать сбор новых данных, аугментацию и переобучение модели.

**A/B тестирование** позволяет сравнивать производительность различных версий моделей в производственных условиях, обеспечивая безопасное развертывание обновлений.

## Заключение

Нейронные сети революционизировали область компьютерного зрения, обеспечив беспрецедентную точность и возможности для широкого спектра задач. Для AI-архитектора понимание принципов работы, архитектур и техник оптимизации нейронных сетей является критически важным для создания эффективных промышленных систем.

Успешное применение нейронных сетей в промышленных условиях требует комплексного подхода, включающего правильный выбор архитектуры, эффективное обучение, оптимизацию для целевой платформы и непрерывный мониторинг производительности. Transfer Learning и техники адаптации домена особенно важны для промышленных применений, где размеченные данные часто ограничены.

Будущее развитие нейронных сетей для компьютерного зрения связано с созданием более эффективных архитектур, улучшением техник обучения с ограниченными данными, и развитием специализированного аппаратного обеспечения. Интеграция с другими технологиями искусственного интеллекта открывает новые возможности для создания более интеллектуальных и адаптивных систем компьютерного зрения.

## Цели обучения
По завершении этого урока вы сможете:
- Понимать основные концепции, представленные в уроке
- Применять полученные знания на практике
- Интегрировать новые навыки в реальные проекты

## Практические задания
1. Изучите представленные примеры кода
2. Выполните практические упражнения
3. Адаптируйте решения под ваши задачи

## Дополнительные материалы
- Документация по используемым технологиям
- Примеры реальных проектов
- Ссылки на дополнительные ресурсы

## Домашнее задание
Выполните практические задания и подготовьтесь к следующему уроку.

---
*Урок 25 модуля 5 курса "AI-Архитектор"*
