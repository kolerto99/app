# Урок 27: Сегментация изображений

## Информация об уроке
- **Модуль:** 5 - Обработка данных и пайплайны
- **Продолжительность:** 170 минут
- **Тип:** Теоретический + Практический

## Содержание урока

# Урок 27: Сегментация изображений

## Введение в сегментацию изображений

Сегментация изображений представляет собой одну из наиболее фундаментальных и сложных задач компьютерного зрения, которая заключается в разделении изображения на семантически значимые области или объекты. Для AI-архитектора глубокое понимание принципов, методов и практических аспектов сегментации является критически важным для создания эффективных промышленных систем визуального анализа, особенно в контексте автоматизированного контроля качества, роботизированного зрения и медицинской диагностики.

В отличие от задач классификации, которые присваивают одну метку всему изображению, или обнаружения объектов, которые локализуют объекты с помощью ограничивающих прямоугольников, сегментация обеспечивает пиксельно-точное понимание содержимого изображения. Каждому пикселю изображения присваивается метка класса, что позволяет получить детальную карту объектов и их границ.

Промышленные применения сегментации изображений охватывают широкий спектр задач: от точного измерения геометрических параметров деталей до анализа дефектов поверхности, от автоматизированного контроля сварных швов до сегментации биологических образцов в фармацевтической промышленности. Успешное внедрение таких систем требует не только технического мастерства в области алгоритмов компьютерного зрения, но и глубокого понимания специфических требований производственных процессов.

Современные подходы к сегментации изображений основаны на глубоких нейронных сетях, которые демонстрируют беспрецедентную точность в выделении сложных объектов и их границ. Однако применение этих технологий в промышленных условиях связано с уникальными вызовами, включая требования к субпиксельной точности, устойчивость к изменениям условий съемки, необходимость обработки в реальном времени и интеграцию с системами автоматизированного принятия решений.

Эволюция методов сегментации прошла путь от простых пороговых методов и алгоритмов роста областей до сложных архитектур глубокого обучения с attention механизмами и трансформерами. Понимание этой эволюции помогает AI-архитектору выбирать наиболее подходящие решения для конкретных промышленных применений, учитывая баланс между точностью, скоростью обработки и вычислительными требованиями.

### Типы сегментации и их применения

Сегментация изображений включает несколько различных типов задач, каждая из которых имеет свои особенности и области применения в промышленных системах.

**Семантическая сегментация** присваивает каждому пикселю изображения метку класса, создавая плотную карту предсказаний. Все пиксели, принадлежащие одному классу, получают одинаковую метку, независимо от того, принадлежат ли они одному или нескольким экземплярам объектов этого класса. Математически семантическая сегментация формулируется как задача плотного предсказания:

f: ℝ^(H×W×C) → {1, 2, ..., K}^(H×W)

где входное изображение размером H×W с C каналами отображается в карту меток размером H×W с K возможными классами.

В промышленных применениях семантическая сегментация используется для задач, где важно выделить области определенного типа, но не требуется различать отдельные экземпляры объектов. Примеры включают сегментацию дефектных областей на поверхности материалов, выделение зон различных материалов в композитных структурах, или идентификацию областей коррозии на металлических поверхностях.

**Сегментация экземпляров (Instance Segmentation)** расширяет семантическую сегментацию, различая отдельные экземпляры объектов одного класса. Каждый пиксель получает не только метку класса, но и идентификатор конкретного экземпляра объекта. Это позволяет точно подсчитывать количество объектов и анализировать их индивидуальные характеристики.

Формально сегментация экземпляров может быть представлена как:

f: ℝ^(H×W×C) → {(c₁, i₁), (c₂, i₂), ..., (cₖ, iₖ)}^(H×W)

где cⱼ - класс объекта, iⱼ - идентификатор экземпляра.

Промышленные применения сегментации экземпляров включают подсчет и анализ отдельных деталей на конвейере, контроль качества упаковки с множественными объектами, анализ клеточных структур в биотехнологии, и роботизированное зрение для манипуляции отдельными объектами.

**Панорамная сегментация (Panoptic Segmentation)** объединяет семантическую сегментацию и сегментацию экземпляров, обеспечивая полное понимание сцены. Каждый пиксель получает как семантическую метку, так и идентификатор экземпляра (если применимо). Панорамная сегментация различает "вещи" (things) - объекты с четкими границами, которые можно подсчитать, и "материалы" (stuff) - аморфные области без четких границ.

**Интерактивная сегментация** позволяет пользователю направлять процесс сегментации через различные формы взаимодействия, такие как щелчки мыши, штрихи или ограничивающие прямоугольники. Это особенно полезно в промышленных применениях, где требуется быстрая адаптация к новым типам объектов или дефектов без полного переобучения модели.

### Математические основы сегментации

Понимание математических принципов, лежащих в основе алгоритмов сегментации, является критически важным для AI-архитектора при проектировании эффективных промышленных систем.

**Функции потерь** играют центральную роль в обучении моделей сегментации. Стандартная кросс-энтропийная потеря для семантической сегментации определяется как:

L_CE = -1/(H×W) ∑ᵢ₌₁ʰ ∑ⱼ₌₁ʷ ∑ₖ₌₁ᴷ yᵢⱼₖ log(p̂ᵢⱼₖ)

где yᵢⱼₖ - истинная метка (one-hot encoded), p̂ᵢⱼₖ - предсказанная вероятность класса k для пикселя (i,j).

**Dice Loss** особенно эффективна для задач с дисбалансом классов и основана на коэффициенте Dice (F1-мере):

Dice = 2|A ∩ B| / (|A| + |B|)

Dice Loss определяется как:

L_Dice = 1 - (2∑ᵢ pᵢgᵢ + ε) / (∑ᵢ pᵢ + ∑ᵢ gᵢ + ε)

где pᵢ - предсказанная вероятность, gᵢ - истинная метка, ε - малая константа для численной стабильности.

**Focal Loss** адаптирована для сегментации для решения проблемы дисбаланса классов:

L_Focal = -α(1-p̂)^γ log(p̂)

где α - весовой коэффициент, γ - параметр фокусировки.

**Tversky Loss** обобщает Dice Loss, позволяя контролировать баланс между ложноположительными и ложноотрицательными предсказаниями:

L_Tversky = 1 - (∑ᵢ pᵢgᵢ + ε) / (∑ᵢ pᵢgᵢ + α∑ᵢ pᵢ(1-gᵢ) + β∑ᵢ (1-pᵢ)gᵢ + ε)

где α и β контролируют штрафы за ложноположительные и ложноотрицательные предсказания соответственно.

**Boundary Loss** специально разработана для улучшения качества границ сегментированных объектов:

L_Boundary = ∫_Ω φ(x) · ∇G(x) dx

где φ(x) - функция расстояния до истинной границы, G(x) - предсказанная вероятность.

## Традиционные методы сегментации

Понимание классических подходов к сегментации остается важным для AI-архитектора, поскольку многие принципы этих методов используются в современных гибридных системах и для предобработки данных.

### Пороговые методы

Пороговая сегментация представляет собой простейший подход к разделению изображения на области на основе интенсивности пикселей.

**Глобальная пороговая сегментация** использует единый порог для всего изображения:

g(x,y) = {1, если f(x,y) ≥ T
         {0, если f(x,y) < T

где f(x,y) - интенсивность пикселя, T - пороговое значение, g(x,y) - бинарное изображение.

Оптимальный порог может быть найден с помощью различных методов:

**Метод Оцу** максимизирует межклассовую дисперсию:

σ²_between(T) = ω₁(T)ω₂(T)[μ₁(T) - μ₂(T)]²

где ω₁, ω₂ - веса классов, μ₁, μ₂ - средние значения классов.

**Адаптивная пороговая сегментация** использует локальные пороги, вычисляемые для каждого пикселя на основе его окрестности:

T(x,y) = μ(x,y) - C

где μ(x,y) - среднее значение в окрестности пикселя (x,y), C - константа.

**Многоуровневая пороговая сегментация** использует несколько порогов для разделения изображения на множественные области:

g(x,y) = {k, если T_{k-1} ≤ f(x,y) < T_k

### Методы роста областей

Алгоритмы роста областей начинают с начальных точек (seeds) и итеративно добавляют соседние пиксели, удовлетворяющие критерию однородности.

**Базовый алгоритм роста областей**:

1. Выбор начальных точек
2. Для каждой начальной точки:
   - Добавить соседние пиксели, удовлетворяющие критерию
   - Обновить параметры области
   - Повторить до сходимости

Критерий однородности может быть основан на различных признаках:

|f(x,y) - μ_region| < T

где μ_region - среднее значение области.

**Разделение и слияние (Split and Merge)** комбинирует подходы сверху-вниз и снизу-вверх:

1. Разделение: рекурсивно разделять области, не удовлетворяющие критерию однородности
2. Слияние: объединять соседние области с похожими свойствами

### Методы на основе границ

Обнаружение границ является фундаментальной задачей для многих алгоритмов сегментации.

**Градиентные операторы** вычисляют производные изображения для обнаружения резких изменений интенсивности:

Оператор Собеля:
G_x = [-1 0 1; -2 0 2; -1 0 1] * I
G_y = [-1 -2 -1; 0 0 0; 1 2 1] * I
|G| = √(G_x² + G_y²)

**Детектор границ Кэнни** включает несколько этапов:

1. Сглаживание гауссовым фильтром
2. Вычисление градиентов
3. Подавление немаксимумов
4. Двойная пороговая фильтрация
5. Трассировка границ по гистерезису

**Активные контуры (Snakes)** минимизируют энергетический функционал:

E = ∫₀¹ [E_internal(v(s)) + E_external(v(s))] ds

где E_internal контролирует гладкость контура, E_external притягивает контур к границам объекта.

### Водораздельная сегментация

Алгоритм водораздела (Watershed) рассматривает изображение как топографическую поверхность и находит линии водораздела между различными бассейнами.

**Классический алгоритм водораздела**:

1. Найти локальные минимумы (маркеры)
2. Имитировать затопление с различных минимумов
3. Построить дамбы в местах встречи различных вод
4. Дамбы образуют границы сегментации

**Маркерно-управляемый водораздел** использует предварительно определенные маркеры для контроля процесса сегментации, что помогает избежать пересегментации.

## Глубокое обучение для сегментации

Революция глубокого обучения кардинально изменила подходы к сегментации изображений, приведя к созданию мощных end-to-end систем с беспрецедентной точностью.

### Полносвязные сверточные сети (FCN)

Полносвязные сверточные сети стали первым успешным применением CNN для семантической сегментации, заменив полносвязные слои классификационных сетей на сверточные слои.

**Архитектура FCN** основана на модификации классификационных сетей:

1. Замена полносвязных слоев на сверточные (1×1 свертки)
2. Добавление слоев деконволюции (transpose convolution) для увеличения разрешения
3. Skip connections для объединения признаков различных уровней

Операция деконволюции (transpose convolution) определяется как:

y[i,j] = ∑ₘ ∑ₙ x[⌊i/s⌋-m, ⌊j/s⌋-n] · w[m,n]

где s - stride, w - веса фильтра.

**Skip connections** объединяют признаки высокого разрешения с низкого уровня с семантически богатыми признаками высокого уровня:

output = upsample(high_level_features) + low_level_features

Это позволяет сохранить пространственные детали при сохранении семантической информации.

### U-Net и его варианты

U-Net стала одной из наиболее влиятельных архитектур для сегментации, особенно в медицинских применениях.

**Архитектура U-Net** состоит из:

1. **Сжимающий путь (Contracting Path)**: последовательность сверточных блоков с max pooling
2. **Расширяющий путь (Expansive Path)**: последовательность up-convolution и конкатенации
3. **Skip connections**: прямые соединения между соответствующими уровнями

Каждый сверточный блок включает:
- Две свертки 3×3 с ReLU активацией
- Batch normalization (в современных вариантах)
- Dropout для регуляризации

**U-Net++** расширяет базовую архитектуру U-Net, добавляя плотные skip connections:

x^{i,j} = H(x^{i-1,j}, U(x^{i,j+1}), x^{i-1,j+1})

где H - сверточный блок, U - операция upsampling.

**Attention U-Net** интегрирует attention механизмы для фокусировки на релевантных признаках:

α = σ(W_α^T(W_x^T x + W_g^T g + b_α) + b_α)

где x - признаки с skip connection, g - gating signal с более глубокого уровня.

### DeepLab семейство

DeepLab представляет семейство архитектур, специально разработанных для семантической сегментации с фокусом на точность границ объектов.

**Atrous (Dilated) Convolution** является ключевой инновацией DeepLab:

y[i] = ∑ₖ x[i + r·k] w[k]

где r - dilation rate, позволяющий увеличить рецептивное поле без увеличения количества параметров.

**Atrous Spatial Pyramid Pooling (ASPP)** применяет параллельные atrous convolutions с различными dilation rates:

ASPP = Concat([Conv1×1, AtrousConv3×3(r=6), AtrousConv3×3(r=12), AtrousConv3×3(r=18), GlobalAvgPool])

**DeepLabv3+** объединяет encoder-decoder архитектуру с ASPP:

1. Encoder с ASPP для извлечения мультимасштабных признаков
2. Decoder для восстановления пространственного разрешения
3. Skip connections для сохранения деталей

### Сегментация экземпляров

Сегментация экземпляров требует одновременного решения задач обнаружения и сегментации объектов.

**Mask R-CNN** расширяет Faster R-CNN, добавляя ветвь для предсказания масок:

1. Backbone сеть для извлечения признаков
2. RPN для генерации предложений объектов
3. RoI Align для точного извлечения признаков регионов
4. Три головы: классификация, регрессия bbox, предсказание маски

**RoI Align** решает проблему неточности RoI Pooling:

RoIAlign(x, roi) = ∑ᵢ ∑ⱼ x(pᵢⱼ) · max(0, 1-|pᵢⱼ - (i,j)|)

где pᵢⱼ - точки билинейной интерполяции.

Функция потерь Mask R-CNN объединяет потери всех задач:

L = L_cls + L_bbox + L_mask

**YOLACT (You Only Look At CoefficienTs)** предлагает real-time подход к сегментации экземпляров:

1. Предсказание коэффициентов прототипов для каждого объекта
2. Генерация прототипов масок для всего изображения
3. Линейная комбинация прототипов с коэффициентами

Финальная маска вычисляется как:

M = σ(PC^T)

где P - прототипы, C - коэффициенты, σ - сигмоидная функция.

### Трансформеры для сегментации

**SETR (SEgmentation TRansformer)** адаптирует Vision Transformer для сегментации:

1. Разделение изображения на патчи
2. Обработка последовательности патчей трансформером
3. Декодер для восстановления пространственного разрешения

**SegFormer** объединяет эффективный трансформер encoder с легковесным MLP decoder:

Encoder: иерархический трансформер с различными разрешениями
Decoder: MLP для объединения мультимасштабных признаков

**Mask2Former** унифицирует семантическую сегментацию, сегментацию экземпляров и панорамную сегментацию:

1. Transformer decoder с обучаемыми запросами объектов
2. Masked attention для фокусировки на релевантных областях
3. Унифицированная функция потерь для всех типов сегментации

## Специализированные техники и применения

Промышленные применения часто требуют специализированных подходов к сегментации, адаптированных к конкретным условиям и требованиям.

### Сегментация в условиях ограниченных данных

Промышленные применения часто сталкиваются с ограниченным количеством размеченных данных, что требует специальных подходов к обучению моделей сегментации.

**Few-shot сегментация** обучает модели сегментировать новые классы объектов с использованием всего нескольких примеров:

Support Set: S = {(x₁, y₁), (x₂, y₂), ..., (xₖ, yₖ)}
Query Image: xᵩ
Task: предсказать yᵩ для xᵩ

**Prototypical Networks** для сегментации вычисляют прототипы классов:

c_k = 1/|S_k| ∑_{(x,y)∈S_k} f_θ(x, y)

где f_θ - feature extractor, S_k - support set для класса k.

**Meta-learning подходы** обучают модели быстро адаптироваться к новым задачам:

θ* = θ - α∇_θ L_task(f_θ)

где α - learning rate, L_task - потеря на конкретной задаче.

**Domain Adaptation** адаптирует модели, обученные на одном домене, к другому домену:

L_total = L_seg + λL_adv

где L_seg - потеря сегментации, L_adv - adversarial потеря для выравнивания доменов.

### Сегментация дефектов и аномалий

Обнаружение и сегментация дефектов представляет особую сложность из-за разнообразия типов дефектов и их редкости.

**Аномальная сегментация** использует модели, обученные только на нормальных данных:

Reconstruction Error: E(x) = ||x - G(E(x))||²

где E - encoder, G - decoder автоэнкодера.

**Variational Autoencoders (VAE)** для сегментации аномалий:

L_VAE = L_reconstruction + βKL(q(z|x)||p(z))

Аномалии обнаруживаются как области с высокой ошибкой реконструкции.

**Adversarial Training** для генерации синтетических дефектов:

L_G = E[log(1 - D(G(z)))] + λL_seg(G(z), y_fake)

где G - генератор дефектов, D - дискриминатор, L_seg - потеря сегментации.

### Мультимодальная сегментация

Промышленные системы часто используют данные с различных сенсоров для улучшения качества сегментации.

**RGB-D сегментация** объединяет цветную информацию с данными глубины:

Feature Fusion: f_fused = Concat([f_RGB, f_Depth])

**Thermal-RGB сегментация** для обнаружения дефектов, невидимых в видимом спектре:

Cross-modal Attention: A = softmax(Q_RGB K_Thermal^T / √d)

**Hyperspectral сегментация** использует спектральную информацию:

Spectral-Spatial Features: f = CNN_spatial(CNN_spectral(x))

### Временная сегментация видео

Сегментация видеопоследовательностей требует учета временной согласованности.

**3D CNN** для видео сегментации:

Conv3D: y[i,j,t] = ∑ₘ ∑ₙ ∑ₜ' x[i+m, j+n, t+t'] · w[m,n,t']

**Optical Flow** для отслеживания движения:

Flow Warping: x_{t+1}^{warped} = Warp(x_t, flow_{t→t+1})

**Recurrent Networks** для временного моделирования:

h_t = LSTM(f_t, h_{t-1})

где f_t - признаки кадра t.

## Оценка качества сегментации

Точная оценка качества сегментации критически важна для промышленных применений, где ошибки могут иметь серьезные последствия.

### Метрики на уровне пикселей

**Pixel Accuracy** измеряет долю правильно классифицированных пикселей:

PA = ∑ᵢ nᵢᵢ / ∑ᵢ ∑ⱼ nᵢⱼ

где nᵢⱼ - количество пикселей класса i, предсказанных как класс j.

**Mean Pixel Accuracy** усредняет точность по классам:

MPA = 1/k ∑ᵢ nᵢᵢ / ∑ⱼ nᵢⱼ

**Intersection over Union (IoU)** для каждого класса:

IoU_i = nᵢᵢ / (∑ⱼ nᵢⱼ + ∑ⱼ nⱼᵢ - nᵢᵢ)

**Mean IoU (mIoU)** усредняет IoU по всем классам:

mIoU = 1/k ∑ᵢ IoU_i

### Метрики на уровне объектов

**Dice Coefficient** особенно важен для медицинских применений:

Dice = 2|A ∩ B| / (|A| + |B|)

**Hausdorff Distance** измеряет максимальное расстояние между границами:

H(A,B) = max{sup_{a∈A} inf_{b∈B} d(a,b), sup_{b∈B} inf_{a∈A} d(a,b)}

**Average Surface Distance** измеряет среднее расстояние между поверхностями:

ASD = 1/(|∂A| + |∂B|) [∑_{a∈∂A} d(a,∂B) + ∑_{b∈∂B} d(b,∂A)]

### Специализированные метрики

**Boundary F1-score** фокусируется на качестве границ:

BF1 = 2 · Precision_boundary · Recall_boundary / (Precision_boundary + Recall_boundary)

**Trimap Accuracy** для интерактивной сегментации:

Trimap_Acc = |correct_pixels_in_unknown_region| / |unknown_region|

**Temporal Consistency** для видео сегментации:

TC = 1 - 1/T ∑ₜ |M_t ⊕ Warp(M_{t-1}, flow_{t-1→t})| / |M_t ∪ Warp(M_{t-1}, flow_{t-1→t})|

## Практические аспекты реализации

Успешное внедрение систем сегментации в промышленных условиях требует учета множества практических факторов.

### Подготовка данных для сегментации

**Стратегии аннотирования** должны обеспечивать высокое качество разметки:

- Полигональная аннотация для точных границ объектов
- Использование специализированных инструментов (CVAT, Labelme, Supervisely)
- Контроль качества с множественными аннотаторами
- Активное обучение для выбора наиболее информативных примеров

**Аугментация данных** для сегментации требует согласованного преобразования изображений и масок:

## Цели обучения
По завершении этого урока вы сможете:
- Понимать основные концепции, представленные в уроке
- Применять полученные знания на практике
- Интегрировать новые навыки в реальные проекты

## Практические задания
1. Изучите представленные примеры кода
2. Выполните практические упражнения
3. Адаптируйте решения под ваши задачи

## Дополнительные материалы
- Документация по используемым технологиям
- Примеры реальных проектов
- Ссылки на дополнительные ресурсы

## Домашнее задание
Выполните практические задания и подготовьтесь к следующему уроку.

---
*Урок 27 модуля 5 курса "AI-Архитектор"*
